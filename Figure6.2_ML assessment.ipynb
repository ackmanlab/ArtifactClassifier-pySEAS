{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./')\n",
    "sys.path.append('/home/feldheimlab/Documents/pySEAS/')\n",
    "%matplotlib inline\n",
    "\n",
    "# key python packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#new name?\n",
    "from ML_classify import splitData\n",
    "\n",
    "# plt.style.use('publication')\n",
    "dataset_path = '/media/feldheimlab/Elements/PLOScompbioPaper/p21_20m/'\n",
    "plt.rcParams['font.size'] = 8\n",
    "\n",
    "save = False\n",
    "save_dir = '/home/feldheimlab/Desktop/figures/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training dataset to see how well it performs on classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataframe\n",
      "------------------------------------\n",
      "                   age  freq_avgsnr  freq_integrate  freq_maxsnr  \\\n",
      "exp_ic                                                             \n",
      "190408_03-04-0000   22     1.193897        0.006304     1.327806   \n",
      "190408_03-04-0001   22     1.422100        0.120683     1.620314   \n",
      "190408_03-04-0002   22     1.296523        0.107689     1.513039   \n",
      "190408_03-04-0003   22     1.172093        0.179745     1.306229   \n",
      "190408_03-04-0004   22     2.093038        0.589769     3.606708   \n",
      "\n",
      "                   freq_maxsnr_freq  freq_range_high  freq_range_low  \\\n",
      "exp_ic                                                                 \n",
      "190408_03-04-0000          4.204482         4.585020        2.973018   \n",
      "190408_03-04-0001          4.585020         0.005325        0.004883   \n",
      "190408_03-04-0002          4.585020         0.005325        0.004883   \n",
      "190408_03-04-0003          2.973018         0.963882        0.371627   \n",
      "190408_03-04-0004          3.242099         0.007530        0.004883   \n",
      "\n",
      "                   freq_rangesz  length  mass_perc  ...  temporal_max  \\\n",
      "exp_ic                                              ...                 \n",
      "190408_03-04-0000      1.612002     6.0   0.148132  ...    110.647317   \n",
      "190408_03-04-0001      0.000442     2.0   0.071009  ...     69.597709   \n",
      "190408_03-04-0002      0.000442     2.0   0.016204  ...     41.335683   \n",
      "190408_03-04-0003      0.592255    12.0   0.129278  ...     60.723479   \n",
      "190408_03-04-0004      0.002648     6.0   0.062347  ...     36.135139   \n",
      "\n",
      "                   temporal_min  temporal_n_freq  temporal_std  \\\n",
      "exp_ic                                                           \n",
      "190408_03-04-0000    -15.531725              2.0     19.607670   \n",
      "190408_03-04-0001    -15.428648              1.0     14.690767   \n",
      "190408_03-04-0002     -9.098360              1.0      9.046784   \n",
      "190408_03-04-0003    -34.306650              3.0      8.348350   \n",
      "190408_03-04-0004    -16.264991              1.0      8.101949   \n",
      "\n",
      "                   threshold_area  threshold_perc  anml  artifact  \\\n",
      "exp_ic                                                              \n",
      "190408_03-04-0000          7638.0        0.965735     0         1   \n",
      "190408_03-04-0001          4178.0        1.000000     0         1   \n",
      "190408_03-04-0002           930.0        1.000000     0         1   \n",
      "190408_03-04-0003         14637.0        0.999659     0         0   \n",
      "190408_03-04-0004          4758.0        0.712062     0         1   \n",
      "\n",
      "                   hemodynamic  movement  \n",
      "exp_ic                                    \n",
      "190408_03-04-0000          1.0       NaN  \n",
      "190408_03-04-0001          1.0       NaN  \n",
      "190408_03-04-0002          1.0       NaN  \n",
      "190408_03-04-0003          NaN       NaN  \n",
      "190408_03-04-0004          1.0       NaN  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "freq_avgsnr            0\n",
      "freq_integrate         0\n",
      "freq_maxsnr            0\n",
      "freq_maxsnr_freq       0\n",
      "freq_range_high        0\n",
      "freq_range_low         0\n",
      "freq_rangesz           0\n",
      "region_centroid_0      0\n",
      "region_centroid_1      0\n",
      "region_eccentricity    0\n",
      "region_extent          0\n",
      "region_majaxis         0\n",
      "region_majmin_ratio    0\n",
      "region_minaxis         0\n",
      "region_orient          0\n",
      "spatial_COMall_x       0\n",
      "spatial_COMall_y       0\n",
      "spatial_avg            0\n",
      "spatial_max            0\n",
      "spatial_min            0\n",
      "spatial_n_domains      0\n",
      "spatial_std            0\n",
      "temporal_autocorr      0\n",
      "temporal_max           0\n",
      "temporal_min           0\n",
      "temporal_n_freq        0\n",
      "temporal_std           0\n",
      "threshold_area         0\n",
      "threshold_perc         0\n",
      "hemodynamic            0\n",
      "movement               0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "path='./data/'\n",
    "classMetricsPath = path + 'training_dataset.tsv'\n",
    "\n",
    "domain_vars =['spatial_min', 'spatial_max', \n",
    "              'region_minaxis', 'threshold_area', 'region_extent', 'threshold_perc', 'region_majaxis', 'region_majmin_ratio', \n",
    "              'temporal_min', \n",
    "              'freq_rangesz']\n",
    "\n",
    "try:\n",
    "    main_data = pd.read_csv(classMetricsPath, sep = '\\t', index_col='exp_ic')\n",
    "    print('Importing dataframe\\n------------------------------------')\n",
    "    #renaming the columns\n",
    "    neural_dict = {}\n",
    "    \n",
    "    for col in main_data.columns:\n",
    "        if \".\" in col:\n",
    "            new_col = col.replace(\".\", \"_\")\n",
    "            neural_dict[col] = new_col\n",
    "\n",
    "    main_data = main_data.rename(columns=neural_dict)\n",
    "    print(main_data.head())\n",
    "except Exception as e:\n",
    "    main_data = pd.DataFrame()\n",
    "    print('Error importing dataFrame')\n",
    "    print('\\t ERROR : ', e)\n",
    "    assert not main_data.empty, 'Check path to metrics dataframe'\n",
    "\n",
    "    \n",
    "droplist = ['anml','artifact','neural','age', 'length', 'spatial_COMdom_x', \n",
    "            'spatial_COMdom_y', 'mass_perc', 'mass_region','mass_total']\n",
    "try:\n",
    "    main_data = main_data.fillna(value=0).copy()\n",
    "    data = main_data.drop(droplist, axis=1).copy()\n",
    "except Exception as e:\n",
    "    data = main_data.fillna(value=0).copy()\n",
    "    print(e)\n",
    "    \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(data.values)\n",
    "data[:] = scaler.transform(data.values)\n",
    "\n",
    "print(data.isnull().sum())\n",
    "print(main_data['neural'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through and see how well the training subsets perform in model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on iteration number 0\n",
      "Working on iteration number 50\n",
      "Working on iteration number 100\n",
      "Working on iteration number 150\n",
      "Working on iteration number 200\n",
      "Working on iteration number 250\n",
      "Working on iteration number 300\n",
      "Working on iteration number 350\n",
      "Working on iteration number 400\n",
      "Working on iteration number 450\n",
      "Working on iteration number 500\n",
      "Working on iteration number 550\n",
      "Working on iteration number 600\n",
      "Working on iteration number 650\n",
      "Working on iteration number 700\n",
      "Working on iteration number 750\n",
      "Working on iteration number 800\n",
      "Working on iteration number 850\n",
      "Working on iteration number 900\n",
      "Working on iteration number 950\n"
     ]
    }
   ],
   "source": [
    "score = []\n",
    "precision = []\n",
    "recall = []\n",
    "signal = []\n",
    "artifact = []\n",
    "\n",
    "truep =[]\n",
    "truen =[]\n",
    "falsep = []\n",
    "falsen = []\n",
    "\n",
    "clas = main_data.loc[:,'artifact'].copy()\n",
    "clas = np.abs(clas - 1)\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 40, max_features = 2)\n",
    "for i in range(n_iter): # 1000 iterations of models\n",
    "    Xlen = len(main_data)\n",
    "    if i%50==0:\n",
    "        print('Working on iteration number {}'.format(i))\n",
    "    X_train, X_test, y_train, y_test = splitData(data.loc[:,domain_vars].copy(),clas)\n",
    "\n",
    "    rnd_clf.fit(X_train, y_train)\n",
    "    y_pred = rnd_clf.predict(X_test)\n",
    "    y_pred_2 = rnd_clf.predict(X_train)\n",
    "    \n",
    "    rnd_score = rnd_clf.score(X_test, y_test)\n",
    "    rnd_precision =  precision_score(y_test, y_pred)\n",
    "    rnd_recall = recall_score(y_test, y_pred)\n",
    "    rnd_signal = (np.sum(((y_pred==1) & (y_test==1))))/(np.sum((y_test==1)))\n",
    "    rnd_artifact = (np.sum(((y_pred==0) & (y_test==0))))/(np.sum(y_test==0))\n",
    "\n",
    "    truep.append(np.sum(y_pred[y_test==1]==y_test[y_test==1]) + np.sum(y_pred_2[y_train==1]==y_train[y_train==1]))\n",
    "    truen.append(np.sum(y_pred[y_test==0]==y_test[y_test==0]) + np.sum(y_pred_2[y_train==0]==y_train[y_train==0]))\n",
    "    falsen.append(np.sum(y_pred[y_test==1]!=y_test[y_test==1]) + np.sum(y_pred_2[y_train==1]!=y_train[y_train==1]))\n",
    "    falsep.append(np.sum(y_pred[y_test==0]!=y_test[y_test==0]) + np.sum(y_pred_2[y_train==0]!=y_train[y_train==0]))\n",
    "\n",
    "    score.append(rnd_score)\n",
    "    precision.append(rnd_precision)\n",
    "    recall.append(rnd_recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random subsets of training dataset, False positive/negative rates\n",
    "\n",
    "The model has low false positive/negative rates for 7 animals.  \n",
    "\n",
    "Conclusion:  A model based on these 7 mice will most likely be effective in proper classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of neural: \n",
      "True positive (neural):  0.9963094829209577\n",
      "False positive (i.e. artifact but classified neural):  0.00369051707904238\n",
      "\n",
      "Accuracy of artifact: \n",
      "True negative (artifact):  0.9839784711762927\n",
      "False negative (i.e. neural but classified artifact):  0.0160215288237074\n"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize = (5, 5))\n",
    "nbin = 15\n",
    "\n",
    "falsebins = np.arange(0, .05, .05/20)\n",
    "truebins = np.arange(0.95, 1., .05/20)\n",
    "\n",
    "average_tp = np.mean(truep)+np.mean(falsep)\n",
    "axs[0][0].hist(truep/average_tp,\n",
    "               bins = truebins, weights=np.ones(n_iter)/n_iter)\n",
    "axs[0][0].vlines(np.mean(truep)/average_tp, \n",
    "                 ymin= 0, ymax = 0.2, color='red')\n",
    "axs[0][0].set_ylabel('percentage of iterations')\n",
    "\n",
    "average_tn = np.mean(truen)+np.mean(falsen)\n",
    "axs[1][1].hist(truen/average_tn, \n",
    "               bins = truebins, weights=np.ones(n_iter)/n_iter)\n",
    "axs[1][1].vlines(np.mean(truen)/average_tn, \n",
    "                 ymin= 0, ymax = .2, color='red')\n",
    "axs[1][1].set_xlabel('accuracy')\n",
    "\n",
    "axs[0][1].hist(falsep/average_tp, \n",
    "               bins = falsebins, weights=np.ones(n_iter)/n_iter) #top right\n",
    "axs[0][1].vlines(np.mean(falsep)/(np.mean(truep)+np.mean(falsep)), \n",
    "                 ymin= 0, ymax = .2, color='red')\n",
    "\n",
    "axs[1][0].hist(falsen/average_tn, \n",
    "               bins = falsebins, weights=np.ones(n_iter)/n_iter) #bottom left\n",
    "axs[1][0].vlines(np.mean(falsen)/average_tn, \n",
    "                 ymin= 0, ymax = .2, color='red')\n",
    "axs[1][0].set_xlabel('accuracy')\n",
    "axs[1][0].set_ylabel('percentage of iterations')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print('\\nAccuracy of neural: ')\n",
    "print('True positive (neural): ', np.mean(truep)/average_tp)\n",
    "print('False positive (i.e. artifact but classified neural): ', np.mean(falsep)/average_tp)\n",
    "\n",
    "print('\\nAccuracy of artifact: ')\n",
    "print('True negative (artifact): ', np.mean(truen)/average_tn)\n",
    "print('False negative (i.e. neural but classified artifact): ', np.mean(falsen)/average_tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test novel dataset on model trained on the full training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataframe\n",
      "------------------------------------\n",
      "                   age  freq_avgsnr  freq_integrate  freq_maxsnr  \\\n",
      "exp_ic                                                             \n",
      "190408_07-08-0000   22          NaN             NaN          NaN   \n",
      "190408_07-08-0001   22     1.061946        0.028321     1.103959   \n",
      "190408_07-08-0002   22     1.080982        0.002786     1.124209   \n",
      "190408_07-08-0003   22     1.037830        0.006557     1.037830   \n",
      "190408_07-08-0004   22     1.071967        0.031053     1.107243   \n",
      "\n",
      "                   freq_maxsnr_freq  freq_range_high  freq_range_low  \\\n",
      "exp_ic                                                                 \n",
      "190408_07-08-0000               NaN              NaN             NaN   \n",
      "190408_07-08-0001          4.585020         0.008955        0.007530   \n",
      "190408_07-08-0002          3.855527         3.242099        1.767767   \n",
      "190408_07-08-0003          5.000000         0.004883        0.004883   \n",
      "190408_07-08-0004          2.726269         2.973018        1.363135   \n",
      "\n",
      "                   freq_rangesz  length  mass_perc  ...  temporal_max  \\\n",
      "exp_ic                                              ...                 \n",
      "190408_07-08-0000           NaN     NaN   0.071172  ...     85.816542   \n",
      "190408_07-08-0001      0.001425     3.0   0.057083  ...     83.266942   \n",
      "190408_07-08-0002      1.474332     8.0   0.113654  ...     34.519882   \n",
      "190408_07-08-0003      0.000000     1.0   0.046910  ...     71.809404   \n",
      "190408_07-08-0004      1.609883    10.0   0.072667  ...     78.787771   \n",
      "\n",
      "                   temporal_min  temporal_n_freq  temporal_std  \\\n",
      "exp_ic                                                           \n",
      "190408_07-08-0000    -15.668996              0.0     17.437513   \n",
      "190408_07-08-0001    -14.493803              2.0     15.931262   \n",
      "190408_07-08-0002    -19.017042              1.0     11.960568   \n",
      "190408_07-08-0003     -9.697240              1.0     11.445192   \n",
      "190408_07-08-0004    -12.153651              3.0     10.742859   \n",
      "\n",
      "                   threshold_area  threshold_perc  anml  artifact  \\\n",
      "exp_ic                                                              \n",
      "190408_07-08-0000          2598.0        0.786558     1         1   \n",
      "190408_07-08-0001          2630.0        0.796728     1         1   \n",
      "190408_07-08-0002          6417.0        0.725331     1         1   \n",
      "190408_07-08-0003          1610.0        0.613801     1         1   \n",
      "190408_07-08-0004          3562.0        0.496931     1         1   \n",
      "\n",
      "                   hemodynamic  movement  \n",
      "exp_ic                                    \n",
      "190408_07-08-0000          1.0       NaN  \n",
      "190408_07-08-0001          1.0       NaN  \n",
      "190408_07-08-0002          1.0       NaN  \n",
      "190408_07-08-0003          1.0       NaN  \n",
      "190408_07-08-0004          1.0       NaN  \n",
      "\n",
      "[5 rows x 41 columns]\n",
      "                   freq_avgsnr  freq_integrate  freq_maxsnr  freq_maxsnr_freq  \\\n",
      "exp_ic                                                                          \n",
      "190408_07-08-0000    -2.297012       -0.592574    -1.136142         -2.505531   \n",
      "190408_07-08-0001    -0.223967       -0.525079    -0.251391          1.160290   \n",
      "190408_07-08-0002    -0.186807       -0.585933    -0.235161          0.577045   \n",
      "190408_07-08-0003    -0.271045       -0.576946    -0.304389          1.492075   \n",
      "190408_07-08-0004    -0.204406       -0.518568    -0.248759         -0.325821   \n",
      "\n",
      "                   freq_range_high  freq_range_low  freq_rangesz  \\\n",
      "exp_ic                                                             \n",
      "190408_07-08-0000        -1.829784       -1.083139     -1.706691   \n",
      "190408_07-08-0001        -1.822494       -1.075755     -1.704564   \n",
      "190408_07-08-0002         0.809327        0.650408      0.494088   \n",
      "190408_07-08-0003        -1.825809       -1.078351     -1.706691   \n",
      "190408_07-08-0004         0.590292        0.253609      0.696429   \n",
      "\n",
      "                   region_centroid_0  region_centroid_1  region_eccentricity  \\\n",
      "exp_ic                                                                         \n",
      "190408_07-08-0000           1.529905           0.148307             0.669741   \n",
      "190408_07-08-0001           1.657169           0.111882             1.136041   \n",
      "190408_07-08-0002           0.214135           0.171251             1.179738   \n",
      "190408_07-08-0003           1.446423           0.158260             0.281479   \n",
      "190408_07-08-0004          -1.132455           0.060437             0.790859   \n",
      "\n",
      "                   ...  spatial_std  temporal_autocorr  temporal_max  \\\n",
      "exp_ic             ...                                                 \n",
      "190408_07-08-0000  ...     0.082900           0.586121      3.588129   \n",
      "190408_07-08-0001  ...     0.082917           0.545540      3.453879   \n",
      "190408_07-08-0002  ...     0.082904           0.596364      0.887090   \n",
      "190408_07-08-0003  ...     0.082914           0.573899      2.850579   \n",
      "190408_07-08-0004  ...     0.082898           0.230587      3.218027   \n",
      "\n",
      "                   temporal_min  temporal_n_freq  temporal_std  \\\n",
      "exp_ic                                                           \n",
      "190408_07-08-0000     -0.958218        -1.778898      5.718523   \n",
      "190408_07-08-0001     -0.742286         0.292430      5.103059   \n",
      "190408_07-08-0002     -1.573392        -0.743234      3.480608   \n",
      "190408_07-08-0003      0.139040        -0.743234      3.270022   \n",
      "190408_07-08-0004     -0.312304         1.328094      2.983044   \n",
      "\n",
      "                   threshold_area  threshold_perc  hemodynamic  movement  \n",
      "exp_ic                                                                    \n",
      "190408_07-08-0000       -0.883123       -0.204278     3.322082 -0.496988  \n",
      "190408_07-08-0001       -0.876703       -0.171166     3.322082 -0.496988  \n",
      "190408_07-08-0002       -0.116977       -0.403608     3.322082 -0.496988  \n",
      "190408_07-08-0003       -1.081330       -0.766703     3.322082 -0.496988  \n",
      "190408_07-08-0004       -0.689731       -1.147185     3.322082 -0.496988  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "freq_avgsnr            0\n",
      "freq_integrate         0\n",
      "freq_maxsnr            0\n",
      "freq_maxsnr_freq       0\n",
      "freq_range_high        0\n",
      "freq_range_low         0\n",
      "freq_rangesz           0\n",
      "region_centroid_0      0\n",
      "region_centroid_1      0\n",
      "region_eccentricity    0\n",
      "region_extent          0\n",
      "region_majaxis         0\n",
      "region_majmin_ratio    0\n",
      "region_minaxis         0\n",
      "region_orient          0\n",
      "spatial_COMall_x       0\n",
      "spatial_COMall_y       0\n",
      "spatial_avg            0\n",
      "spatial_max            0\n",
      "spatial_min            0\n",
      "spatial_n_domains      0\n",
      "spatial_std            0\n",
      "temporal_autocorr      0\n",
      "temporal_max           0\n",
      "temporal_min           0\n",
      "temporal_n_freq        0\n",
      "temporal_std           0\n",
      "threshold_area         0\n",
      "threshold_perc         0\n",
      "hemodynamic            0\n",
      "movement               0\n",
      "dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "classMetricsPath = path + 'novel_dataset.tsv'\n",
    "\n",
    "try:\n",
    "    novel_data = pd.read_csv(classMetricsPath, sep = '\\t', index_col='exp_ic')\n",
    "    print('Importing dataframe\\n------------------------------------')\n",
    "    #renaming the columns\n",
    "    neural_dict = {}\n",
    "    \n",
    "    for col in novel_data.columns:\n",
    "        if \".\" in col:\n",
    "            new_col = col.replace(\".\", \"_\")\n",
    "            neural_dict[col] = new_col\n",
    "\n",
    "    novel_data = novel_data.rename(columns=neural_dict)\n",
    "    print(novel_data.head())\n",
    "except Exception as e:\n",
    "    novel_data = pd.DataFrame()\n",
    "    print('Error importing dataFrame')\n",
    "    print('\\t ERROR : ', e)\n",
    "    assert not novel_data.empty, 'Check path to metrics dataframe'\n",
    "\n",
    "    \n",
    "droplist = ['anml','artifact','neural','age', 'length', 'spatial_COMdom_x', \n",
    "            'spatial_COMdom_y', 'mass_perc', 'mass_region','mass_total']\n",
    "try:\n",
    "    novel_data = novel_data.fillna(value=0).copy()\n",
    "    test_data = novel_data.drop(droplist, axis=1).copy()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(test_data.values)\n",
    "test_data[:] = scaler.transform(test_data.values)\n",
    "\n",
    "print(test_data.head())\n",
    "\n",
    "print(test_data.isnull().sum())\n",
    "print(novel_data['neural'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on ONLY the 7 animals previously tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan ... nan nan nan]\n",
      "Series([], dtype: float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_671586/3825556307.py:12: RuntimeWarning: Mean of empty slice\n",
      "  print(np.nanmean(classConfidence.values, axis = 1))\n"
     ]
    }
   ],
   "source": [
    "X_train = main_data.loc[:,domain_vars].fillna(0).copy() #ALL DATA from training dataset\n",
    "scaler.fit(X_train.values)\n",
    "X_train[:] = scaler.transform(X_train.values)\n",
    "\n",
    "y_train = main_data.loc[:,'artifact'].copy() #ALL CLASSIFICATIONS from training dataset\n",
    "y_train = np.abs(y_train - 1) #swap from artifact being positive to it being negative [0: artifact; 1: neural signal]\n",
    "\n",
    "\n",
    "classConfidence = pd.DataFrame(index=main_data.index) #store confidence based on the full model\n",
    "\n",
    "\n",
    "print(np.nanmean(classConfidence.values, axis = 1))\n",
    "print(classConfidence.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the above model on NEW datasets that were NOT used in training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on iteration number 0\n",
      "Working on iteration number 50\n",
      "Working on iteration number 100\n",
      "Working on iteration number 150\n",
      "Working on iteration number 200\n",
      "Working on iteration number 250\n",
      "Working on iteration number 300\n",
      "Working on iteration number 350\n",
      "Working on iteration number 400\n",
      "Working on iteration number 450\n",
      "Working on iteration number 500\n",
      "Working on iteration number 550\n",
      "Working on iteration number 600\n",
      "Working on iteration number 650\n",
      "Working on iteration number 700\n",
      "Working on iteration number 750\n",
      "Working on iteration number 800\n",
      "Working on iteration number 850\n",
      "Working on iteration number 900\n",
      "Working on iteration number 950\n",
      "['190408_07-08', '190423_03-04', '190508_03-04', '190508_07-08', '200110_01-02']\n",
      "                   anml  artifact  neural  count  percent\n",
      "exp_ic                                                   \n",
      "190408_07-08-0000     1         1       0      0    0.000\n",
      "190408_07-08-0001     1         1       0      0    0.000\n",
      "190408_07-08-0002     1         1       0      0    0.000\n",
      "190408_07-08-0003     1         1       0     31    0.031\n",
      "190408_07-08-0004     1         1       0     39    0.039\n"
     ]
    }
   ],
   "source": [
    "iterations = 1000 #number of random times to fit the model\n",
    "nanimals = np.unique(novel_data['anml']) #New animals NOT used in training\n",
    "\n",
    "classConfidence = pd.DataFrame(index=main_data.index, columns=np.arange(0,iterations)) #store confidence based on the full model\n",
    "# \n",
    "\n",
    "novel_results = novel_data[['anml','artifact']].copy() # create a dataframe to store results from model fit\n",
    "novel_results['neural'] = np.abs(novel_results['artifact'] - 1)\n",
    "novel_results['count'] = 0\n",
    "\n",
    "novel_true_pos = np.zeros((iterations, len(nanimals)))\n",
    "novel_true_neg = np.zeros((iterations, len(nanimals)))\n",
    "novel_false_pos = np.zeros((iterations, len(nanimals)))\n",
    "novel_false_neg = np.zeros((iterations, len(nanimals)))\n",
    "explist = []\n",
    "\n",
    "prec = np.zeros((iterations, len(nanimals)))\n",
    "reca = np.zeros((iterations, len(nanimals)))\n",
    "accu = np.zeros((iterations, len(nanimals)))\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators = 40, max_features = 2)#model\n",
    "for j in range(iterations):\n",
    "    rnd_clf.fit(X_train, y_train)# model fit \n",
    "    classConfidence.loc[X_train.index, j] = rnd_clf.predict_proba(X_train)[:,1] #store confidence values\n",
    "    \n",
    "    if j%50==0:\n",
    "        print('Working on iteration number {}'.format(j))\n",
    "    for i, a in enumerate(nanimals):\n",
    "        #test each idividual animal performance\n",
    "        subset_test = novel_data.loc[novel_data['anml']==a, domain_vars].copy() \n",
    "\n",
    "        subset_test[:] = scaler.transform(subset_test.values)# scale based on ONLY this 1 animal dataset         \n",
    "        y_test = novel_data.loc[novel_data['anml']==a, 'artifact'].copy()\n",
    "        y_test = np.abs(y_test - 1)\n",
    "        \n",
    "        if j == 0:#establish indices based on the first round\n",
    "            explist.append(subset_test.index[0][:12])\n",
    "\n",
    "        y_pred = rnd_clf.predict(subset_test) #predict classification based on the model\n",
    "\n",
    "        #metrics based on model fit\n",
    "        accu[j,i] = rnd_clf.score(subset_test, y_test)\n",
    "        prec[j,i] = precision_score(y_test, y_pred)\n",
    "        reca[j,i] = recall_score(y_test, y_pred)\n",
    "        \n",
    "        novel_true_pos[j,i]= np.sum(y_pred[y_test==1]==y_test[y_test==1])\n",
    "        novel_true_neg[j,i]= np.sum(y_pred[y_test==0]==y_test[y_test==0])\n",
    "        novel_false_neg[j,i]= np.sum(y_pred[y_test==1]!=y_test[y_test==1])\n",
    "        novel_false_pos[j,i]= np.sum(y_pred[y_test==0]!=y_test[y_test==0])\n",
    "        novel_results.loc[subset_test.index, 'count'] += y_pred\n",
    "\n",
    "print(explist)\n",
    "novel_results['percent'] = novel_results['count']/iterations\n",
    "print(novel_results.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classConfidencePath = './data/20min_P21_confidence.tsv'\n",
    "classConfidence.to_csv(classConfidencePath, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot confidence scores based on classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2190, 2)\n",
      "2190\n"
     ]
    }
   ],
   "source": [
    "#Plotting TruncatedSVD Plot\n",
    "# ---------------------------------------------------------\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=2, n_iter=100, random_state=42)\n",
    "\n",
    "X_reduced_SVD = svd.fit_transform(X_train)\n",
    "print(X_reduced_SVD.shape)\n",
    "\n",
    "# distribution of confidences\n",
    "nbins = 10\n",
    "bins = np.linspace(0,1,nbins+1)\n",
    "\n",
    "hist_dist_all = np.zeros((len(classConfidence.columns), nbins))\n",
    "for i, col in enumerate(classConfidence.columns):\n",
    "    hist_dist_all[i], bin_edges1 = np.histogram(classConfidence.loc[main_data.index, col], bins)\n",
    "meanbyindex = classConfidence.mean(axis=1)\n",
    "\n",
    "hist_dist_n = np.zeros((len(classConfidence.columns), nbins))\n",
    "for i, col in enumerate(classConfidence.columns):\n",
    "    hist_dist_n[i], bin_edges1 = np.histogram(classConfidence.loc[main_data[main_data['artifact']==0].index, col], bins)\n",
    "\n",
    "hist_dist_v = np.zeros((len(classConfidence.columns), nbins))\n",
    "for i, col in enumerate(classConfidence.columns):\n",
    "    hist_dist_v[i], bin_edges1 = np.histogram(classConfidence.loc[main_data[main_data['hemodynamic']==1].index, col], bins)\n",
    "    \n",
    "hist_dist_o = np.zeros((len(classConfidence.columns), nbins))\n",
    "for i, col in enumerate(classConfidence.columns):\n",
    "    hist_dist_o[i], bin_edges1 = np.histogram(classConfidence.loc[main_data[main_data['movement']==1].index, col], bins)\n",
    "\n",
    "print(len(meanbyindex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (4,2), constrained_layout=False)\n",
    "\n",
    "gs = fig.add_gridspec(4,8)\n",
    "gs.update(wspace = 0.15, hspace = 1)\n",
    "\n",
    "ax0 = fig.add_subplot(gs[:3, :4])\n",
    "ax2 = fig.add_subplot(gs[:3, 4:])\n",
    "\n",
    "main_data.loc[main_data['artifact']==0, 'class'] = 5\n",
    "main_data.loc[main_data['hemodynamic']==1, 'class'] = 0\n",
    "main_data.loc[main_data['movement']==1, 'class'] = 1\n",
    "\n",
    "colors = np.zeros((len(main_data), 4)) #map type component\n",
    "for i, ind in enumerate(main_data.index):\n",
    "    if main_data.loc[ind, 'class'] == 0:\n",
    "        colors[i] = plt.cm.get_cmap('RdYlBu',6)(0)\n",
    "    elif main_data.loc[ind, 'class'] == 1:\n",
    "        colors[i] = plt.cm.get_cmap('RdYlBu',6)(1)\n",
    "    elif main_data.loc[ind, 'class'] == 5:\n",
    "        colors[i] = plt.cm.get_cmap('RdYlBu',6)(5)  \n",
    "\n",
    "ph = ax0.scatter(X_reduced_SVD[:, 0], X_reduced_SVD[:, 1], \n",
    "                 facecolors=\"None\",\n",
    "                 edgecolors=colors,\n",
    "                 lw=0.25,\n",
    "                 s = 3)\n",
    "\n",
    "ax0.set_ylim([-3,8])\n",
    "ax0.set_xlim([-6,10])\n",
    "ax0.set_title('')\n",
    "ax0.set_aspect(1)\n",
    "ax0.set_xticks([-4,0,4,8])\n",
    "ax0.set_yticks([0,4,8])\n",
    "\n",
    "\n",
    "colors = np.zeros((len(main_data), 4))\n",
    "for i, ind in enumerate(main_data.index): #map average confidence as artifact (red)/neural(blue) of each component\n",
    "    colors[i] = plt.cm.get_cmap('RdYlBu')(meanbyindex[i])\n",
    "\n",
    "ph3 = ax2.scatter(X_reduced_SVD[:, 0], X_reduced_SVD[:, 1], \n",
    "                 facecolors=\"None\",\n",
    "                 edgecolors=colors,\n",
    "                 lw=0.25,\n",
    "                 s = 3)\n",
    "\n",
    "ax2.set_ylim(ax0.get_ylim())\n",
    "ax2.set_xlim(ax0.get_xlim())\n",
    "ax2.set_yticklabels('')\n",
    "ax2.set_xticks([-4,0,4,8])\n",
    "ax2.set_yticks([0,4,8])\n",
    "ax2.set_aspect(1)\n",
    "\n",
    "# plt.tight_layout()\n",
    "if save: \n",
    "    plt.savefig('SVD_plots_confidence_OB.svg', dpi = 600)\n",
    "plt.show()\n",
    "\n",
    "# print(np.array(hist1)/sum(hist1))\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "highest neuro classified:  70.67013698630142  +/-  0.178999068083504\n",
      "highest artifact classified:  23.31269406392694  +/-  0.2115377616387997\n",
      "highest vasc classified:  7.197077625570775  +/-  0.18104630697037963\n",
      "highest other classified:  16.115616438356174  +/-  0.10598503041312365\n"
     ]
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2,1,figsize=(2,0.75))\n",
    "\n",
    "allv = len(meanbyindex)\n",
    "binpos = bin_edges1[:-1] + (bin_edges1[1:] - bin_edges1[:-1])/2\n",
    "width = (bin_edges1[1:] - bin_edges1[:-1])[0]\n",
    "\n",
    "axs[0].bar(binpos, np.mean(hist_dist_n/allv*100, axis=0), \n",
    "           yerr = np.std(hist_dist_n/allv*100, axis=0), \n",
    "           width = width,\n",
    "            color=plt.cm.get_cmap('RdYlBu',6)(5)\n",
    "          )\n",
    "axs[1].bar(binpos,np.mean(hist_dist_v/allv*100, axis=0),\n",
    "           yerr = np.sqrt(np.std(hist_dist_v/allv*100, axis=0)), \n",
    "           width = width,\n",
    "            color=plt.cm.get_cmap('RdYlBu',6)(0)\n",
    "          )\n",
    "bottoms = np.mean(hist_dist_v/allv*100, axis=0)\n",
    "axs[1].bar(binpos, np.mean(hist_dist_o/allv*100, axis=0),\n",
    "           yerr = np.std(hist_dist_o/allv*100, axis=0),  \n",
    "           width = width,\n",
    "           bottom=bottoms,\n",
    "            color=plt.cm.get_cmap('RdYlBu',6)(1)\n",
    "          )\n",
    "\n",
    "axs[0].spines['right'].set_visible(False)\n",
    "axs[0].spines['top'].set_visible(False)\n",
    "# axs[1].spines['bottom'].set_visible(False)\n",
    "axs[0].set_ylim(0.1,100)\n",
    "axs[0].set_xlim([1,0])\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].set_yticks([1,100])\n",
    "axs[0].set_yticklabels([1,100])\n",
    "axs[0].set_xticks([0,.5,1.00])\n",
    "axs[0].set_xticklabels([])\n",
    "\n",
    "axs[1].spines['right'].set_visible(False)\n",
    "axs[1].spines['top'].set_visible(False)\n",
    "# axs[2].spines['bottom'].set_visible(False)\n",
    "axs[1].set_ylim(0.1,25)\n",
    "axs[1].set_xlim([1,0])\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].set_yticks([1,100])\n",
    "axs[1].set_yticklabels([1,100])\n",
    "axs[1].set_xticks([0,.5,1.00])\n",
    "axs[1].set_xticklabels([])\n",
    "print('highest neuro classified: ', np.mean(hist_dist_n/allv*100, axis=0)[9], ' +/- ', \n",
    "           np.std(hist_dist_n/allv*100, axis=0)[9])\n",
    "print('highest artifact classified: ', np.mean((hist_dist_v+hist_dist_o)/allv*100, axis=0)[0], ' +/- ', \n",
    "           np.std((hist_dist_v+hist_dist_o)/allv*100, axis=0)[0])\n",
    "print('highest vasc classified: ', np.mean(hist_dist_v/allv*100, axis=0)[0], ' +/- ', \n",
    "           np.std(hist_dist_v/allv*100, axis=0)[0])\n",
    "print('highest other classified: ', np.mean(hist_dist_o/allv*100, axis=0)[0], ' +/- ', \n",
    "           np.std(hist_dist_o/allv*100, axis=0)[0])\n",
    "if save:\n",
    "    fig.savefig('classifier_performance_mat_hist.svg', dpi=600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize NOVEL data performance, based on each animal performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.DataFrame(columns = explist, index = np.arange(iterations))\n",
    "pdf[:] = prec*100\n",
    "rdf = pdf.copy()\n",
    "rdf[:] = reca*100\n",
    "adf = pdf.copy()\n",
    "adf[:] = accu*100\n",
    "\n",
    "pdf = pdf.melt()\n",
    "pdf['metric'] = 'precision'\n",
    "\n",
    "rdf = rdf.melt()\n",
    "rdf['metric'] = 'recall'\n",
    "\n",
    "adf = adf.melt()\n",
    "adf['metric'] = 'accuracy'\n",
    "\n",
    "metric_data = pd.concat([adf, pdf, rdf], ignore_index =True)\n",
    "print(len(metric_data))\n",
    "print(len(pdf))\n",
    "\n",
    "import seaborn as sns\n",
    "sns.boxplot(x='metric', y='value', hue='variable', data=metric_data)\n",
    "plt.ylim([90,100])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy mean:  0.9824921483771811\n",
      "Precision mean:  0.994544890241628\n",
      "Recall mean:  0.9811990233816056\n"
     ]
    }
   ],
   "source": [
    "data1 = [np.array(score)*100, np.array(precision)*100, np.array(recall)*100]\n",
    "data2 = [np.array(accu)*100, np.array(prec)*100, np.array(reca)*100]\n",
    "\n",
    "boxplotprops = {'boxprops':dict(linewidth=1, color='k'), \n",
    "    'medianprops':dict(linewidth=1, color='k'), \n",
    "    'whiskerprops':dict(linestyle='-', color='k'), \n",
    "    'capprops':dict(color='k'),\n",
    "   'flierprops':dict(markerfacecolor='k', marker='.', linewidth=0.5, markersize = 2)\n",
    "    }\n",
    "\n",
    "import matplotlib\n",
    "colors = matplotlib.cm.get_cmap('tab20c')\n",
    "import numpy as np\n",
    "\n",
    "current_class = [96, 97, 96]\n",
    "fig = plt.figure(figsize=(3,2))\n",
    "\n",
    "axs0 = plt.subplot2grid((1,3),(0,0))\n",
    "axs1 = plt.subplot2grid((1,3),(0,1), colspan=2)\n",
    "\n",
    "axs0.boxplot(data1, widths = 0.65, **boxplotprops)# flierprops=flier)#, boxprops=boxprops, medianprops=medianprops, notch=False, widths=0.75)\n",
    "axs0.set_ylim([90, 101])\n",
    "axs0.set_xticklabels(['score', 'precision', 'recall'])\n",
    "axs0.set_yticks([90, 95, 100])\n",
    "axs0.set_ylabel('percent')\n",
    "axs0.minorticks_on()\n",
    "axs0.tick_params(axis='x', which='minor', bottom=False)\n",
    "\n",
    "im = sns.boxplot(x='metric', y='value', hue='variable', data=metric_data,\n",
    "           ax = axs1, fliersize = 0.5)#, palette = grayify_cmap('jet'))\n",
    "\n",
    "axs1.set_ylim([90, 101])\n",
    "# ax[1].set_yticks([])\n",
    "axs1.set_ylabel('')\n",
    "\n",
    "axs1.minorticks_on()\n",
    "axs1.set_xticks([0, 1, 2])\n",
    "axs1.set_xticklabels(['score', 'precision', 'recall'])\n",
    "plt.legend([])\n",
    "axs1.set_yticks([90, 95, 100])\n",
    "axs1.tick_params(axis='x', which='minor', bottom=False)\n",
    "axs1.set_xlabel('')\n",
    "for i,box in enumerate(im.artists):\n",
    "    box.set_edgecolor('black')\n",
    "    box.set_facecolor('white')\n",
    "    box.set_linewidth(1)\n",
    "    # iterate over whiskers and median lines\n",
    "    for j in range(6*i,6*(i+1)):\n",
    "        im.lines[j].set_color('black')\n",
    "        im.lines[j].set_linewidth(1)\n",
    "# if save:\n",
    "#     plt.savefig(save_dir+'Novel_data_comparison_20min_Correction.svg', dpi = 600)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Accuracy mean: ', np.mean(accu))\n",
    "print('Precision mean: ', np.mean(prec))\n",
    "print('Recall mean: ', np.mean(reca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classical Accuracy mean:  0.9769056316590563  +/-  0.00554858113957651\n",
      "Classical Precision mean:  0.9877228043253001  +/-  0.004935591633613628\n",
      "Classical Recall mean:  0.9806735966735969  +/-  0.0064174803744625145\n",
      "\n",
      "Novel Accuracy mean:  0.9824921483771811  +/-  0.006634121712614391\n",
      "Novel Precision mean:  0.994544890241628  +/-  0.006319011854805275\n",
      "Novel Recall mean:  0.9811990233816056  +/-  0.005295191882285346\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "across training/testing set of animals (classical machine learning where a subset \n",
    "components are used to train classification of components of the same dataset)\n",
    "'''\n",
    "print('Classical Accuracy mean: ', np.mean(score), ' +/- ', np.std(score))\n",
    "print('Classical Precision mean: ', np.mean(precision), ' +/- ', np.std(precision))\n",
    "print('Classical Recall mean: ', np.mean(recall), ' +/- ', np.std(recall))\n",
    "    \n",
    "'''\n",
    "training on other animals and then tested on a NOVEL group of animals\n",
    "'''\n",
    "print('\\nNovel Accuracy mean: ', np.mean(accu), ' +/- ', np.std(accu))\n",
    "print('Novel Precision mean: ', np.mean(prec), ' +/- ', np.std(prec))\n",
    "print('Novel Recall mean: ', np.mean(reca), ' +/- ', np.std(reca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall correct: 98.25%\n",
      "Overall incorrect: 1.75%\n",
      "\n",
      "Overall HIGHLY confident and correct: 93.62%\n",
      "Overall HIGHLY confident and incorrect: 0.06%\n",
      "\n",
      "Class type:  n\n",
      "Class correct: 98.07% 1171.0/1194\n",
      "Class incorrect: 1.93% 23.0/1194\n",
      "\n",
      "Class type:  v\n",
      "Class correct: 98.46% 128.0/130\n",
      "Class incorrect: 1.54% 2.0/130\n",
      "\n",
      "Class type:  o\n",
      "Class correct: 99.36% 311.0/313\n",
      "Class incorrect: 0.64% 2.0/313\n"
     ]
    }
   ],
   "source": [
    "novel_results.loc[novel_results['percent']==novel_results['neural'] ,'alwaysTP_TN'] = 1 #HIGHLY confident true positive/negative\n",
    "novel_results.loc[novel_results['percent']==novel_results['artifact'] ,'alwaysFP_FN'] = 1 \n",
    "novel_results.loc[novel_results['percent']==0 ,'alwaysTN_FN'] = 1 #HIGLY confident true negative/false negative\n",
    "novel_results.loc[novel_results['percent']==1 ,'alwaysTP_FP'] = 1 #HIGLY confident true positive/false positive\n",
    "\n",
    "novel_results['rounded_all'] = np.round(novel_results['percent']) #based on confidence, how would the model assign class\n",
    "novel_results.loc[novel_results['rounded_all']==novel_results['neural'] ,'TP_TN'] = 1 #HIGHLY confident true positive/negative\n",
    "novel_results.loc[novel_results['rounded_all']==novel_results['artifact'] ,'FP_FN'] = 1 #HIGHLY confident true positive/negative\n",
    "novel_results.loc[novel_results['rounded_all']==0 ,'TN_FN'] = 1 #HIGLY confident true negative/false negative\n",
    "novel_results.loc[novel_results['rounded_all']==1 ,'TP_FP'] = 1 #HIGLY confident true positive/false positive\n",
    "\n",
    "novel_data.loc[novel_data['hemodynamic']==1, 'class'] = 'v' #define the types of class\n",
    "novel_data.loc[novel_data['movement']==1, 'class'] = 'o'\n",
    "novel_data.loc[novel_data['artifact']==0, 'class'] = 'n'\n",
    "novel_results['class'] = novel_data['class']\n",
    "\n",
    "print('Overall correct: {}%'.format(\n",
    "    np.round(np.nansum(novel_results['TP_TN'])/len(novel_results)*100, 2)))\n",
    "print('Overall incorrect: {}%'.format(\n",
    "    np.round(np.nansum(novel_results['FP_FN'])/len(novel_results)*100, 2)))\n",
    "\n",
    "print('\\nOverall HIGHLY confident and correct: {}%'.format(\n",
    "    np.round(np.nansum(novel_results['alwaysTP_TN'])/len(novel_results)*100, 2)))\n",
    "print('Overall HIGHLY confident and incorrect: {}%'.format(\n",
    "    np.round(np.nansum(novel_results['alwaysFP_FN'])/len(novel_results)*100, 2)))\n",
    "\n",
    "for c in ['n','v','o']:\n",
    "    print('\\nClass type: ', c)\n",
    "    index = novel_results[novel_results['class']==c].index\n",
    "    print('Class correct: {0}% {1}/{2}'.format(\n",
    "        np.round(np.nansum(novel_results.loc[index, 'TP_TN'])/len(index)*100, 2),\n",
    "        np.nansum(novel_results.loc[index, 'TP_TN']),len(index)))\n",
    "    print('Class incorrect: {0}% {1}/{2}'.format(\n",
    "        np.round(np.nansum(novel_results.loc[index, 'FP_FN'])/len(index)*100, 2),\n",
    "        np.nansum(novel_results.loc[index, 'FP_FN']),len(index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = np.unique(novel_data['anml'])\n",
    "n_components = np.zeros((len(animals)))\n",
    "\n",
    "for i, anml in enumerate(animals):\n",
    "    animal = novel_data[novel_data['anml']==anml]\n",
    "#     n_components[i] = int(animal.index[-1][-4:])\n",
    "    n_components[i] = len(animal)\n",
    "    for n, j in enumerate(animal.index[::-1]):\n",
    "        novel_data.loc[j,'var_pos'] = int(j[-3:])\n",
    "        novel_data.loc[j,'rel_pos'] = int(j[-3:])/n_components[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190408_07-08\n",
      "190423_03-04\n",
      "190508_03-04\n",
      "190508_07-08\n",
      "200110_01-02\n"
     ]
    }
   ],
   "source": [
    "novel_data['region_centroid_1_ud'] = (novel_data['region_centroid_1']-np.max(novel_data['region_centroid_1'])/2)*6.75/1000\n",
    "novel_data['region_centroid_0_ud'] = novel_data['region_centroid_0']*6.75/1000\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(4,2))\n",
    "\n",
    "\n",
    "for i, a in enumerate(nanimals):\n",
    "    subset = novel_data[novel_data['anml']==a].copy()\n",
    "    result = novel_results[novel_results['anml']==a].copy()\n",
    "    print(subset.index[0][:12])\n",
    "    if i ==0:\n",
    "        binn = 10\n",
    "        P = np.histogram(result.loc[subset['class']=='n', 'percent'],binn)\n",
    "        bins = P[1]\n",
    "        binpos = P[1][:-1]+(P[1][1:]-P[1][:-1])/2\n",
    "        per_val = np.zeros((len(nanimals),3,binn))\n",
    "\n",
    "    else:\n",
    "        P = np.histogram(result.loc[subset['class']=='n', 'percent'],bins)\n",
    "    axs[0].scatter(binpos, np.ones(len(P[0]))*-i*2, s = P[0]/len(subset)*100, color=plt.cm.get_cmap('RdYlBu',6)(5))\n",
    "    per_val[i,0] = P[0]\n",
    "    N = np.histogram(result.loc[(subset['class']=='v'), 'percent'],bins=bins)\n",
    "    axs[0].scatter(binpos, np.ones(len(N[0]))*-1*i*2-11.5, s = N[0]/len(subset)*100, color=plt.cm.get_cmap('RdYlBu',6)(0))\n",
    "    per_val[i,1] = N[0]\n",
    "    N = np.histogram(result.loc[subset['class']=='o', 'percent'],bins=P[1])\n",
    "    axs[0].scatter(binpos, np.ones(len(N[0]))*-1*i*2-12.5, s = N[0]/len(subset)*100, color= plt.cm.get_cmap('RdYlBu',6)(1))\n",
    "    per_val[i,2] = N[0]\n",
    "axs[0].set_xlim(1,0)\n",
    "axs[0].set_yticks([0,-2,-4,-6,-8,-12,-14,-16,-18,-20])\n",
    "axs[0].set_yticklabels([])\n",
    "axs[0].set_xticklabels([])\n",
    "\n",
    "\n",
    "axs[1].scatter(np.abs(novel_data.loc[(novel_results['rounded_all']!=novel_results['neural'])&(novel_data['class']=='n'), 'region_centroid_1_ud']),\n",
    "            novel_data.loc[(novel_results['rounded_all']!=novel_results['neural'])&(novel_data['class']=='n'), 'region_centroid_0_ud'],\n",
    "           color=plt.cm.get_cmap('RdYlBu',6)(5), s=2)\n",
    "axs[1].scatter(-1*np.abs(novel_data.loc[(novel_results['rounded_all']!=novel_results['neural'])&(novel_data['class']=='v'), 'region_centroid_1_ud']),\n",
    "            novel_data.loc[(novel_results['rounded_all']!=novel_results['neural'])&(novel_data['class']=='v'), 'region_centroid_0_ud'],\n",
    "           color=plt.cm.get_cmap('RdYlBu',6)(0), s=2)\n",
    "axs[1].scatter(-1*np.abs(novel_data.loc[(novel_results['rounded_all']!=novel_results['neural'])&(novel_data['class']=='o'), 'region_centroid_1_ud']),\n",
    "            novel_data.loc[(novel_results['rounded_all']!=novel_results['neural'])&(novel_data['class']=='o'), 'region_centroid_0_ud'],\n",
    "           color=plt.cm.get_cmap('RdYlBu',6)(1), s=2)\n",
    "\n",
    "axs[1].set_xlim(novel_data['region_centroid_1_ud'].max()+0.5,novel_data['region_centroid_1_ud'].min()-0.5)\n",
    "axs[1].set_ylim(novel_data['region_centroid_0_ud'].min()-0.5, novel_data['region_centroid_0_ud'].max()+0.5)\n",
    "axs[1].set_aspect(1)\n",
    "axs[1].axis('off')\n",
    "plt.savefig('Novel_data_comparison_confusion_mat_loc_OB.svg', dpi = 600)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.38714991762768\n",
      "2.6274576039459143\n",
      "highest neuro classified:  95.38714991762768  +/-  2.6274576039459148\n",
      "highest artifact classified:  1.2355848434925865  +/-  0.1727856422026609\n",
      "highest vascular classified:  0.9060955518945634  +/-  0.0807080640126253\n",
      "highest other classified:  0.3294892915980231  +/-  0.09606181045873644\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams['font.size'] = 8\n",
    "fig, axs = plt.subplots(2,1,figsize=(2,0.75))\n",
    "\n",
    "print(np.max(np.sum(per_val[:,0,:], axis = 0)/np.sum(per_val)*100))\n",
    "print(np.std(per_val[:,0,:]/np.sum(per_val)*100, axis = 0)[-1])\n",
    "\n",
    "# axs[0].bar(binpos, np.sum(per_val[:,0,:], axis = 0)/np.sum(per_val)*100, width = 0.1,\n",
    "#            yerr =  np.std(per_val[:,0,:]/np.sum(per_val)*100, axis = 0),\n",
    "#            color=plt.cm.get_cmap('RdYlBu',6)(5))\n",
    "axs[0].bar(binpos, np.sum(per_val[:,0,:], axis = 0)/np.sum(per_val)*100, width = 0.1,\n",
    "           yerr =  np.std(per_val[:,0,:]/np.sum(per_val)*100, axis = 0),\n",
    "           color=plt.cm.get_cmap('RdYlBu',6)(5))\n",
    "axs[1].bar(binpos, np.sum(per_val[:,1,:], axis = 0)/np.sum(per_val)*100, width = 0.1,\n",
    "           yerr =  np.std(per_val[:,1,:]/np.sum(per_val)*100, axis = 0),\n",
    "           color=plt.cm.get_cmap('RdYlBu',6)(0))\n",
    "bottoms = np.sum(per_val[:,1,:], axis = 0)/np.sum(per_val)*100\n",
    "axs[1].bar(binpos, np.sum(per_val[:,2,:], axis = 0)/np.sum(per_val)*100, width = 0.1, \n",
    "           yerr =  np.std(per_val[:,2,:]/np.sum(per_val)*100, axis = 0),\n",
    "           bottom=bottoms,\n",
    "        color=plt.cm.get_cmap('RdYlBu',6)(1))\n",
    "\n",
    "print('highest neuro classified: ', np.sum(per_val[:,0,9], axis = 0)/np.sum(per_val)*100, ' +/- ', \n",
    "           np.std(per_val[:,0,9], axis = 0)/np.sum(per_val)*100)\n",
    "print('highest artifact classified: ', np.sum(per_val[:,1,0] + per_val[:,2,0], axis = 0)/np.sum(per_val)*100, ' +/- ', \n",
    "           np.std(per_val[:,1,0] + per_val[:,2,0], axis = 0)/np.sum(per_val)*100)\n",
    "print('highest vascular classified: ', np.sum(per_val[:,1,0], axis = 0)/np.sum(per_val)*100, ' +/- ', \n",
    "           np.std(per_val[:,1,0], axis = 0)/np.sum(per_val)*100)\n",
    "print('highest other classified: ', np.sum(per_val[:,2,0], axis = 0)/np.sum(per_val)*100, ' +/- ', \n",
    "           np.std(per_val[:,2,0], axis = 0)/np.sum(per_val)*100)\n",
    "\n",
    "axs[0].set_xticklabels([])\n",
    "axs[0].spines['right'].set_visible(False)\n",
    "axs[0].spines['top'].set_visible(False)\n",
    "# axs[1].spines['bottom'].set_visible(False)\n",
    "axs[0].set_ylim(0.1,100)\n",
    "axs[0].set_xlim([1,0])\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "axs[1].spines['right'].set_visible(False)\n",
    "axs[1].spines['top'].set_visible(False)\n",
    "# axs[2].spines['bottom'].set_visible(False)\n",
    "axs[1].set_ylim(.1,100)\n",
    "axs[1].set_xlim([1,0])\n",
    "axs[1].set_yscale('log')\n",
    "\n",
    "if save:\n",
    "    fig.savefig('classifier_performance_mat_novel_hist.svg', dpi=600)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
